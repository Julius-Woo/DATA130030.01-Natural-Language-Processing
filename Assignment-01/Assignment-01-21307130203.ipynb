{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186371d-a854-4609-8158-8537db05c369",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "> 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "> 2. The deadline of Assignment-01 is 23:59pm, 03-31-2024\n",
    ">\n",
    "> 3. In this assignment, you will\n",
    ">    1) explore Wikipedia text data\n",
    ">    2) build language models\n",
    ">    3) build NB and LR classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23f2dd-e01e-4b02-9b17-c92885f8a428",
   "metadata": {},
   "source": [
    "## Task0 - Download datasets\n",
    "> Download the preprocessed data, enwiki-train.json and enwiki-test.json from the Assignment-01 folder. In the data file, each line contains a Wikipedia page with attributes, title, label, and text. There are 1000 records in the train file and 100 records in test file with ten categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc7cb5-dd64-4886-8cd5-24f147288941",
   "metadata": {},
   "source": [
    "## Task1 - Data exploring and preprocessing\n",
    "\n",
    "> 1) Print out how many documents are in each class  (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ba34ae-48ae-438a-b3dd-c9d4cb3ee416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document counts per class in enwiki-train.json:\n",
      "Film: 100\n",
      "Book: 100\n",
      "Politician: 100\n",
      "Writer: 100\n",
      "Food: 100\n",
      "Actor: 70\n",
      "Animal: 80\n",
      "Software: 130\n",
      "Artist: 100\n",
      "Disease: 120\n",
      "-----------------------\n",
      "Document counts per class in enwiki-test.json:\n",
      "Film: 10\n",
      "Book: 10\n",
      "Politician: 10\n",
      "Writer: 10\n",
      "Food: 10\n",
      "Actor: 10\n",
      "Animal: 10\n",
      "Software: 10\n",
      "Artist: 10\n",
      "Disease: 10\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "def get_class_count(data, dataset_type):\n",
    "    \"\"\"Get the number of documents for each class.\"\"\"\n",
    "    class_cnts = defaultdict(int)\n",
    "    for record in data:\n",
    "        class_cnts[record['label']] += 1\n",
    "    print(f\"Document counts per class in {dataset_type}:\")\n",
    "    for label, count in class_cnts.items():\n",
    "        print(f\"{label}: {count}\")\n",
    "\n",
    "\n",
    "train_data = load_data('enwiki-train.json')\n",
    "test_data = load_data('enwiki-test.json')\n",
    "\n",
    "get_class_count(train_data, \"enwiki-train.json\")\n",
    "print(\"-----------------------\")\n",
    "get_class_count(test_data, \"enwiki-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a2d5c-b719-41ec-8f14-04e092517eb9",
   "metadata": {},
   "source": [
    "> 2) Print out the average number of sentences in each class.\n",
    ">    You may need to use sentence tokenization of NLTK.\n",
    ">    (for both train and test dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e85dc7-d50b-406a-9550-b2063f236ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sentences per class in enwiki-train.json:\n",
      "Film: 438.56\n",
      "Book: 400.36\n",
      "Politician: 706.2\n",
      "Writer: 420.32\n",
      "Food: 175.24\n",
      "Actor: 76.7\n",
      "Animal: 70.375\n",
      "Software: 260.95384615384614\n",
      "Artist: 306.47\n",
      "Disease: 404.9\n",
      "-----------------------\n",
      "Average number of sentences per class in enwiki-test.json:\n",
      "Film: 364.7\n",
      "Book: 295.9\n",
      "Politician: 597.6\n",
      "Writer: 294.9\n",
      "Food: 107.6\n",
      "Actor: 30.7\n",
      "Animal: 46.8\n",
      "Software: 160.1\n",
      "Artist: 234.0\n",
      "Disease: 311.7\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def get_avg_sen(data, dataset_type):\n",
    "    \"\"\"Get the average number of words per class.\"\"\"\n",
    "    class_sen_cnts = defaultdict(list)\n",
    "    for record in data:\n",
    "        sentences = sent_tokenize(record['text'])  # split text into sentences\n",
    "        class_sen_cnts[record['label']].append(len(sentences))\n",
    "    print(f\"Average number of sentences per class in {dataset_type}:\")\n",
    "    for label, counts in class_sen_cnts.items():\n",
    "        avg_sen = sum(counts) / len(counts) if len(counts) > 0 else 0\n",
    "        print(f\"{label}: {avg_sen}\")\n",
    "\n",
    "# train_data = load_data('enwiki-train.json')\n",
    "# test_data = load_data('enwiki-test.json')\n",
    "\n",
    "get_avg_sen(train_data, \"enwiki-train.json\")\n",
    "print(\"-----------------------\")\n",
    "get_avg_sen(test_data, \"enwiki-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef409-65a5-4d07-9b69-c5986569970a",
   "metadata": {},
   "source": [
    "> 3) Print out the average number of tokens in each class\n",
    ">    (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d7628e-762c-49fe-804a-796fa0265af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per class in enwiki-train.json:\n",
      "Film: 11895.28\n",
      "Book: 10540.51\n",
      "Politician: 18644.3\n",
      "Writer: 11849.91\n",
      "Food: 3904.15\n",
      "Actor: 1868.8428571428572\n",
      "Animal: 1521.925\n",
      "Software: 6302.3\n",
      "Artist: 8212.91\n",
      "Disease: 9322.958333333334\n",
      "-----------------------\n",
      "Average number of tokens per class in enwiki-test.json:\n",
      "Film: 9292.9\n",
      "Book: 7711.1\n",
      "Politician: 15204.3\n",
      "Writer: 8499.4\n",
      "Food: 2445.5\n",
      "Actor: 677.5\n",
      "Animal: 885.6\n",
      "Software: 3972.8\n",
      "Artist: 5706.4\n",
      "Disease: 6988.8\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_avg_tokens(data, dataset_type):\n",
    "    \"\"\"Get the average number of tokens per class.\"\"\"\n",
    "    class_token_cnts = defaultdict(list)\n",
    "    for record in data:\n",
    "        tokens = word_tokenize(record['text'])\n",
    "        class_token_cnts[record['label']].append(len(tokens))\n",
    "    print(f\"Average number of tokens per class in {dataset_type}:\")\n",
    "    for label, counts in class_token_cnts.items():\n",
    "        avg_tokens = sum(counts) / len(counts) if len(counts) > 0 else 0\n",
    "        print(f\"{label}: {avg_tokens}\")\n",
    "\n",
    "# train_data = load_data('enwiki-train.json')\n",
    "# test_data = load_data('enwiki-test.json')\n",
    "\n",
    "get_avg_tokens(train_data, \"enwiki-train.json\")\n",
    "print(\"-----------------------\")\n",
    "get_avg_tokens(test_data, \"enwiki-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999a66-977c-469b-939c-c3934210972e",
   "metadata": {},
   "source": [
    "> 4) For each sentence in the document, remove punctuations and other special characters so that each sentence only contains English words and numbers. To make your life easier, you can make all words as lower cases. For each class, print out the first article's name and the processed first 40 words. (for both train and test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "819a28ef-0821-4ace-be70-fb9b249d355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Information for each class in enwiki-train.json:\n",
      "Class name: Film\n",
      "The first article's title: Citizen_Kane\n",
      "Processed first 40 words: citizen kane is a 1941 american drama film produced by directed by and starring orson welles he also cowrote the screenplay with herman j mankiewicz the picture was welles first feature film citizen kane is considered by many critics and \n",
      "\n",
      "Class name: Book\n",
      "The first article's title: The_Spirit_of_the_Age\n",
      "Processed first 40 words: the spirit of the age full title the spirit of the age or contemporary portraits is a collection of character sketches by the early 19th century english essayist literary critic and social commentator william hazlitt portraying 25 men mostly british \n",
      "\n",
      "Class name: Politician\n",
      "The first article's title: Charles_de_Gaulle\n",
      "Processed first 40 words: charles andr joseph marie de gaulle 22 november 18909 november 1970 was a french army officer and statesman who led free france against nazi germany in world war ii and chaired the provisional government of the french republic from 1944 \n",
      "\n",
      "Class name: Writer\n",
      "The first article's title: Mircea_Eliade\n",
      "Processed first 40 words: mircea eliade april 22 1986 was a romanian historian of religion fiction writer philosopher and professor at the university of chicago he was a leading interpreter of religious experience who established paradigms in religious studies that persist to this day \n",
      "\n",
      "Class name: Food\n",
      "The first article's title: Korean_cuisine\n",
      "Processed first 40 words: korean cuisine has evolved through centuries of social and political change originating from ancient agricultural and nomadic traditions in korea and southern manchuria korean cuisine reflects a complex interaction of the natural environment and different cultural trends korean cuisine is \n",
      "\n",
      "Class name: Actor\n",
      "The first article's title: Roman_Polanski\n",
      "Processed first 40 words: roman polanski born raymond thierry liebling on 18 august 1933 is a polishfrench film director producer screenwriter and actor his polishjewish parents moved the family from paris back to krakw in 1937 two years later poland was invaded by nazi \n",
      "\n",
      "Class name: Animal\n",
      "The first article's title: Oesophagostomum\n",
      "Processed first 40 words: oesophagostomum is a genus of parasitic nematodes roundworms of the family strongylidae these worms occur in africa brazil china indonesia and the philippines the majority of human infection with oesophagostomum is localized to northern togo and ghana because the eggs \n",
      "\n",
      "Class name: Software\n",
      "The first article's title: Android_(operating_system)\n",
      "Processed first 40 words: android is a mobile operating system based on a modified version of the linux kernel and other open source software designed primarily for touchscreen mobile devices such as smartphones and tablets android is developed by a consortium of developers known \n",
      "\n",
      "Class name: Artist\n",
      "The first article's title: Mihai_Olos\n",
      "Processed first 40 words: mihai olos born 26 february 1940 in arini romania died 22 february 2015 in amoltern endigen germany was a romanian conceptual artist poet essayist a gifted colorist in his first paintings he became more attracted towards experimenting with various forms \n",
      "\n",
      "Class name: Disease\n",
      "The first article's title: Domestic_violence\n",
      "Processed first 40 words: domestic violence also called domestic abuse or family violence is violence or other abuse that occurs in a domestic setting such as in a marriage or cohabitation domestic violence is often used as a synonym for intimate partner violence which \n",
      "\n",
      "-----------------------\n",
      "\n",
      "## Information for each class in enwiki-test.json:\n",
      "Class name: Film\n",
      "The first article's title: Monty_Python's_Life_of_Brian\n",
      "Processed first 40 words: monty pythons life of brian also known as life of brian is a 1979 british comedy film starring and written by the comedy group monty python graham chapman john cleese terry gilliam eric idle terry jones and michael palin it \n",
      "\n",
      "Class name: Book\n",
      "The first article's title: Cousin_Bette\n",
      "Processed first 40 words: la cousine bette cousin bette is an 1846 novel by french author honor de balzac set in mid19thcentury paris it tells the story of an unmarried middleaged woman who plots the destruction of her extended family bette works with valrie \n",
      "\n",
      "Class name: Politician\n",
      "The first article's title: Olusegun_Obasanjo\n",
      "Processed first 40 words: chief olusegun matthew okikiola aremu obasanjo gcfr born 5 march 1937 is a nigerian political and military leader who served as nigerias head of state from 1976 to 1979 and later as its president from 1999 to 2007 ideologically a \n",
      "\n",
      "Class name: Writer\n",
      "The first article's title: Horia_Gârbea\n",
      "Processed first 40 words: horiarzvan grbea or grbea born august 10 1962 is a romanian playwright poet essayist novelist and critic also known as an academic engineer and journalist known for his work in experimental theater and his postmodernist contributions to romanian literature he \n",
      "\n",
      "Class name: Food\n",
      "The first article's title: Sponge_cake\n",
      "Processed first 40 words: sponge cake is a light cake made with egg whites flour and sugar sometimes leavened with baking powder sponge cakes leavened with beaten eggs originated during the renaissance possibly in spain the sponge cake is thought to be one of \n",
      "\n",
      "Class name: Actor\n",
      "The first article's title: Kom_Chuanchuen\n",
      "Processed first 40 words: akom preedakul 5 january 1958 30 april 2021 known by stage name kom chuanchuen was a thai comedian and actor best known from comedic supporting roles in thai movies and television originally a nightclub comedian kom made a breakthrough in \n",
      "\n",
      "Class name: Animal\n",
      "The first article's title: Articulata_hypothesis\n",
      "Processed first 40 words: the articulata hypothesis is the grouping in a higher taxon of animals with segmented bodies consisting of annelida and panarthropoda this theory states that these groups are descended from a common segmented ancestor the articulata hypothesis is an alternative to \n",
      "\n",
      "Class name: Software\n",
      "The first article's title: Unix\n",
      "Processed first 40 words: unix trademarked as unix is a family of multitasking multiuser computer operating systems that derive from the original atampt unix whose development started in 1969 at the bell labs research center by ken thompson dennis ritchie and others initially intended \n",
      "\n",
      "Class name: Artist\n",
      "The first article's title: Camille_Pissarro\n",
      "Processed first 40 words: camille pissarro 10 july 1830 13 november 1903 was a danishfrench impressionist and neoimpressionist painter born on the island of st thomas now in the us virgin islands but then in the danish west indies his importance resides in his \n",
      "\n",
      "Class name: Disease\n",
      "The first article's title: Staphylococcus_aureus\n",
      "Processed first 40 words: staphylococcus aureus is a grampositive roundshaped bacterium a member of the firmicutes and is a usual member of the microbiota of the body frequently found in the upper respiratory tract and on the skin it is often positive for catalase \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import re\n",
    "\n",
    "def clean_sen(sentence):\n",
    "    \"\"\"Clean a sentence by removing special characters and converting to lower case.\"\"\"\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)  # remove special characters\n",
    "    sentence = sentence.lower()  # convert to lower case\n",
    "    return sentence\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Clean texts in the dataset.\"\"\"\n",
    "    for record in data:\n",
    "        sentences = sent_tokenize(record['text'])\n",
    "        cleaned_sen = [clean_sen(sen) for sen in sentences]\n",
    "        record['text'] = cleaned_sen\n",
    "    return data\n",
    "\n",
    "def article_print(data, dataset_type):\n",
    "    \"\"\"Join sentence list into text and print the first article's title and the first 40 words for each class.\"\"\"\n",
    "    first_articles = {}  # key is class name, value is (title, first 40 words)\n",
    "    for record in data:\n",
    "        label = record['label']\n",
    "        record['text'] = ' '.join(record['text'])\n",
    "        if label not in first_articles:\n",
    "            tokens = word_tokenize(record['text'])\n",
    "            first_articles[label] = (record['title'], tokens[:40])\n",
    "\n",
    "    print(f\"## Information for each class in {dataset_type}:\")\n",
    "    for label, (title, tokens) in first_articles.items():\n",
    "        print(f\"Class name: {label}\")\n",
    "        print(f\"The first article's title: {title}\")\n",
    "        print(\"Processed first 40 words:\", ' '.join(tokens), \"\\n\")\n",
    "\n",
    "train_data = load_data('enwiki-train.json')\n",
    "test_data = load_data('enwiki-test.json')\n",
    "cleaned_train_data = clean_data(train_data)\n",
    "cleaned_test_data = clean_data(test_data)\n",
    "\n",
    "\n",
    "article_print(cleaned_train_data, \"enwiki-train.json\")\n",
    "print(\"-----------------------\\n\")\n",
    "article_print(cleaned_test_data, \"enwiki-test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c7ea9-18fa-43f1-bded-625c09a947b1",
   "metadata": {},
   "source": [
    "## Task2 - Build language models\n",
    "\n",
    "> 1) Based on the training dataset, build unigram, bigram, and trigram language models using Add-one smoothing technique. It is encouraged to implement models by yourself. If you use public code, please cite it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc6950",
   "metadata": {},
   "source": [
    "The following code borrows ideas from the demo code provided in class, and is modified to fit the requirements of this assignment, with necessary simplifications for the issue of unknown ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e979b1-235f-4663-8acc-00257735e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes to here\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import math\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "def preprocess_text(data, n, freq_threshold=1):\n",
    "    \"\"\"Preprocess text data from the cleaned dataset.\"\"\"\n",
    "    tokenized_sentences = []\n",
    "    # Add SOS and EOS tokens for n-grams\n",
    "    sos = \"<s> \" * (n-1) if n > 1 else \"<s> \"\n",
    "    for record in data:\n",
    "        sentences = sent_tokenize(record['text'])\n",
    "        for sen in sentences:\n",
    "            sen = re.sub(r'[^a-zA-Z0-9\\s]', '', sen).lower()\n",
    "            tokenized_sentences.append(f'{sos}{sen} </s>'.split())\n",
    "    tokens = [token for sentence in tokenized_sentences for token in sentence]\n",
    "    # Replace tokens with low frequency with <UNK>\n",
    "    vocab = FreqDist(tokens)\n",
    "    tokens = [token if vocab[token] > freq_threshold else \"<UNK>\" for token in tokens]\n",
    "    \n",
    "    return tokenized_sentences, tokens\n",
    "\n",
    "\n",
    "class NgramModel:\n",
    "    def __init__(self, tokenized_sentences, train_tokens, n, laplace=1):\n",
    "        \"\"\"Initialize the N-gram model.\"\"\"\n",
    "        self.n = n\n",
    "        self.toked_sen = tokenized_sentences\n",
    "        self.tokens = train_tokens\n",
    "        self.laplace = laplace\n",
    "        self.vocab = FreqDist(self.tokens)  # contains <UNK>, <s>, </s>\n",
    "        self.model = self.create_model()\n",
    "        self.special_words = ['<s>', '</s>', '<UNK>']\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"Create the N-gram model with Add-one smoothing.\"\"\"\n",
    "        model = {}\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        if self.n == 1:  # unigram model\n",
    "            total_tokens = len(self.tokens)\n",
    "            for token, count in self.vocab.items():\n",
    "                model[(token,)] = (count + 1) / \\\n",
    "                    (total_tokens + vocab_size)  # Add-one smoothing\n",
    "        else:\n",
    "            n_gram_list = [\n",
    "                ngram for sent in self.toked_sen for ngram in nltk.ngrams(sent, self.n)]\n",
    "            n_gram_freq = FreqDist(n_gram_list)\n",
    "            n_1_gram_list = [\n",
    "                n_1_gram for sent in self.toked_sen for n_1_gram in nltk.ngrams(sent, self.n-1)]\n",
    "            n_minus1_gram_freq = FreqDist(n_1_gram_list)\n",
    "\n",
    "            for n_gram, freq in n_gram_freq.items():\n",
    "                # The (n-1)-gram is the n-gram minus its last token\n",
    "                n_minus_1_gram = n_gram[:-1]\n",
    "                n_minus_1_gram_count = n_minus1_gram_freq[n_minus_1_gram]\n",
    "                # Calculate the probability with Add-one smoothing\n",
    "                prob = (freq + self.laplace) / (n_minus_1_gram_count + self.laplace * vocab_size)\n",
    "                model[n_gram] = prob\n",
    "            \n",
    "        return model\n",
    "\n",
    "    def perplexity_calc(self, te_toked_sen, te_tokens):\n",
    "        \"\"\"Calculate the perplexity of the N-gram model against a given testset.\"\"\"\n",
    "        # Replace unknown tokens with <UNK>\n",
    "        te_toked_sen = [[token if token in self.vocab else \"<UNK>\" for token in sent]\n",
    "                        for sent in te_toked_sen]\n",
    "        test_ngrams = [\n",
    "            ngram for sent in te_toked_sen for ngram in nltk.ngrams(sent, self.n)]\n",
    "        # exclude <s> for each sentence\n",
    "        N = len(te_tokens) - len(te_toked_sen) * (self.n-1)\n",
    "        logprob = 0\n",
    "        vocab_size = len(self.vocab)\n",
    "        if self.n > 1:\n",
    "            n_1_gram_list = [\n",
    "                n_1_gram for sent in self.toked_sen for n_1_gram in nltk.ngrams(sent, self.n-1)]\n",
    "            n_minus_1_gram_freq = FreqDist(n_1_gram_list)\n",
    "        for ngram in test_ngrams:\n",
    "            if ngram in self.model:\n",
    "                logprob += math.log(self.model[ngram])\n",
    "            else:\n",
    "                n_minus1_gram = ngram[:-1]\n",
    "                if n_minus1_gram in n_minus_1_gram_freq:\n",
    "                    n_minus_1_gram_count = n_minus_1_gram_freq[n_minus1_gram]\n",
    "                    logprob += math.log(self.laplace /\n",
    "                                        (n_minus_1_gram_count + vocab_size * self.laplace))\n",
    "                else:\n",
    "                    logprob += math.log(self.laplace / vocab_size)\n",
    "\n",
    "        return math.exp(-logprob / N)\n",
    "    \n",
    "    \n",
    "    def _best_candidate(self, prev, i=0, without=[]):\n",
    "        \"\"\"Choose the i-th best candidate token given the previous (n-1)-token context.\"\"\"\n",
    "        blacklist = [\"<UNK>\"] + without  # Tokens to avoid\n",
    "        # Generate candidates based on the provided context, filtering out unwanted tokens.\n",
    "        candidates = [\n",
    "            (ngram[-1], prob) for ngram, prob in self.model.items()\n",
    "            if ngram[:-1] == prev and ngram[-1] not in blacklist\n",
    "        ]\n",
    "        \n",
    "        # Sort candidates based on their probability, in descending order.\n",
    "        candidates.sort(key=lambda candidate: candidate[1], reverse=True)\n",
    "        # If no candidates found, return EOS with probability 1.\n",
    "        if not candidates:\n",
    "            return (\"</s>\", 1)\n",
    "        # Select the best candidate; to ensure variety for the first word, select i-th best candidate.\n",
    "        return candidates[0 if prev != () and prev[-1] != \"<s>\" else random.randint(0, len(candidates)-1)]\n",
    "\n",
    "    def generate_sentences(self, num, min_len=20, max_len=40):\n",
    "        \"\"\"Generate sentences using the N-gram model.\"\"\"\n",
    "        for i in range(num):\n",
    "            sent = [\"<s>\"] * max(1, self.n - 1)  # Start each sentence with SOS tokens\n",
    "            log_sent_prob = 0  # Initialize sentence probability\n",
    "            \n",
    "            # Generate tokens until an EOS token is produced or max length is reached\n",
    "            while sent[-1] != \"</s>\" and len(sent) < max_len:\n",
    "                prev = tuple(sent[-(self.n-1):]) if self.n != 1 else ()  # Current context\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])  # Tokens to avoid, guranatee min_len\n",
    "                next_token, next_prob = self._best_candidate(prev, i, without=blacklist)\n",
    "                \n",
    "                sent.append(next_token)  # Add next token to sentence\n",
    "                log_sent_prob += -math.log(next_prob)  # Update sentence probability\n",
    "\n",
    "            # Append </s> if not already done and sentence is too long\n",
    "            if sent[-1] != \"</s>\":\n",
    "                sent.append(\"</s>\")\n",
    "\n",
    "            # Yield the generated sentence and its negative log-probability\n",
    "            yield ' '.join(sent), log_sent_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6edc5-78c5-4de0-8855-2a455d625c97",
   "metadata": {},
   "source": [
    "> 2) Report the perplexity of these 3 trained models on the testing dataset and explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3acda9a7-606c-4fa6-918a-9ebe7745ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for 1-gram model has been completed.\n",
      "1-gram model has been created.\n",
      "Vocabulary size: 43971\n",
      "Perplexity of 1-gram model: 1079.569\n",
      "\n",
      "Preprocessing for 2-gram model has been completed.\n",
      "2-gram model has been created.\n",
      "Vocabulary size: 43971\n",
      "Perplexity of 2-gram model: 3282.316\n",
      "\n",
      "Preprocessing for 3-gram model has been completed.\n",
      "3-gram model has been created.\n",
      "Vocabulary size: 43971\n",
      "Perplexity of 3-gram model: 16081.709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "# Preprocess the text data\n",
    "train_data = load_data('enwiki-train.json')\n",
    "test_data = load_data('enwiki-test.json')\n",
    "laplace = 1\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    train_toked_sen, train_tokens = preprocess_text(\n",
    "        train_data, n, freq_threshold=5)\n",
    "    test_toked_sen, test_tokens = preprocess_text(\n",
    "        test_data, n, freq_threshold=0)\n",
    "    print(f\"Preprocessing for {n}-gram model has been completed.\")\n",
    "    if n == 1:\n",
    "        unigram_model = NgramModel(train_toked_sen, train_tokens, n, laplace)\n",
    "        lm = unigram_model\n",
    "    elif n == 2:\n",
    "        bigram_model = NgramModel(train_toked_sen, train_tokens, n, laplace)\n",
    "        lm = bigram_model\n",
    "    else:\n",
    "        trigram_model = NgramModel(train_toked_sen, train_tokens, n, laplace)\n",
    "        lm = trigram_model\n",
    "    print(f\"{n}-gram model has been created.\")\n",
    "    print(f\"Vocabulary size: {len(lm.vocab)}\")\n",
    "    print(f\"Perplexity of {n}-gram model: {lm.perplexity_calc(test_toked_sen, test_tokens):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675d8c4",
   "metadata": {},
   "source": [
    "From the results above, we can see that as $n$ increases, the perplexity of the model increases. This is contrary to the expectation that the perplexity should decrease as $n$ increases.\n",
    "\n",
    "This may be due to the fact that we use the Add-one smoothing technique, which assigns a non-zero probability to unseen ngrams. This may lead to a higher perplexity as $n$ increases, as the number of unseen ngrams increases. And in the trigram model, when n-1 grams are not seen in the training data, meaning that even with Add-one smoothing, we cannot assign a non-zero probability to the trigram, so we simply assign $1/|V|$ to this trigram, which is perhaps not a good way to handle this situation. A more sophisticated smoothing technique such as Kneser-Ney smoothing may be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd75b35-134d-4629-ac87-ed84fcc9d8e4",
   "metadata": {},
   "source": [
    "> 3) Use each built model to generate five sentences and explain these generated patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea6e1244-2b15-4845-a83b-5708797569b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Generated sentences using the unigram model:\n",
      "Sentence 1: <s> peculiarities 5000 separated jann albumin 163 weston huntingdon devolved flee opponent etiology corporal sully negative accuser relentless maheshwari 205 cel occasion narrow 89th 1960 whirlpool upnp kato everett standoff messages paume deposition textures spinoza caesareans tangerine valuable unei jai </s> (negative log probability: 486.47565)\n",
      "Sentence 2: <s> fundulea deste ltpoemgt profanity closeup braga neurosurgeon nuova uncharted energy jane cutlets iatrogenic shipbuilding kitty incredulous teenagers unprepared deserts osteoarthritis giacchino beadseller midoctober barred commissars tunku stalinism pels unquestionably gains croquetas frere grownups gym proclaimed confluence treaties 20082009 blossomed </s> (negative log probability: 500.58020)\n",
      "Sentence 3: <s> smiles pivotal veto tweed temperament cruise materialize leroy solidify mom assessing evade baxter countrywide derogatory counterattacked reimann atonic endless toxoplasmosis entrepreneur tofu andes mackintosh imparted plasmacytomas contributions laos zmeura enzyme dryer phospholipids insight friction individuals lactate erstwhile annette philosopher </s> (negative log probability: 487.13274)\n",
      "Sentence 4: <s> disraelis mecfs replacements chevalier loaves compilers criminality thinness shuttle selfdeclared rodrigo suppliers afterward rampa fiveyearold vulcan deities humility schemes transports blends miracles uziel palatine samsung dargis mar mayoral forging avoid cassandra tacit gimson recreated nude robson indiana abdomen circadian </s> (negative log probability: 491.71244)\n",
      "Sentence 5: <s> burnside sages claudel willy pc cheddar coded robs marquee 2023 penny beso aphasia classifying intermediary cards etchings dan retailed conferences toll overhearing tombs outnumbered whatsapps mvsxa agha brow melanocytes frere khao bred obscured playwright interesting affairs mwakikagiles unwanted banners </s> (negative log probability: 489.30366)\n",
      "\n",
      "## Generated sentences using the bigram model:\n",
      "Sentence 1: <s> observers to the film was a new york city of his own and in which he had been found that it is not be used for example </s> (negative log probability: 126.04594)\n",
      "Sentence 2: <s> sabarimala temple of the film was a new york city and his own work on to be used in which he had been found that it is not only one </s> (negative log probability: 143.31228)\n",
      "Sentence 3: <s> 159 days later in the film was a new york city of his own and that he had been found to be used for example is not only one </s> (negative log probability: 140.62391)\n",
      "Sentence 4: <s> ibv and the film was a new york city of his own work on to be used in which he had been found that it is not only one </s> (negative log probability: 135.16016)\n",
      "Sentence 5: <s> zodia scafandrului sign of the film was a new york city and his own work on to be used in which he had been found that it is not only one </s> (negative log probability: 149.48223)\n",
      "\n",
      "## Generated sentences using the trigram model:\n",
      "Sentence 1: <s> <s> adobes ceo shantanu narayen responded by saying that the film was released on october 31 2014 a new version of windows 8 </s> (negative log probability: 178.28566)\n",
      "Sentence 2: <s> <s> survivors were loaded into the house of commons on 4 february 1738 and given a new version was released in november 2019 </s> (negative log probability: 188.89495)\n",
      "Sentence 3: <s> <s> juvenals caustic satire was read and write in the united states on november 22 1963 often referred to as a result of this is not known </s> (negative log probability: 202.35966)\n",
      "Sentence 4: <s> <s> leading up to the united states and canada on november 22 1963 often referred by netapp was version 825 </s> (negative log probability: 161.89805)\n",
      "Sentence 5: <s> <s> roverandom and smith was a member of the film as well in his own work on it to be an effective treatment for people with bpd </s> (negative log probability: 201.29592)\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "num_to_gen = 5\n",
    "print(\"\\n## Generated sentences using the unigram model:\")\n",
    "for i, (sentence, prob) in enumerate(unigram_model.generate_sentences(num_to_gen), 1):\n",
    "    print(f\"Sentence {i}: {sentence} (negative log probability: {prob:.5f})\")\n",
    "\n",
    "print(\"\\n## Generated sentences using the bigram model:\")\n",
    "for i, (sentence, prob) in enumerate(bigram_model.generate_sentences(num_to_gen), 1):\n",
    "    print(f\"Sentence {i}: {sentence} (negative log probability: {prob:.5f})\")\n",
    "\n",
    "print(\"\\n## Generated sentences using the trigram model:\")\n",
    "for i, (sentence, prob) in enumerate(trigram_model.generate_sentences(num_to_gen), 1):\n",
    "    print(f\"Sentence {i}: {sentence} (negative log probability: {prob:.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45f5b8",
   "metadata": {},
   "source": [
    "The sentences generated by the unigram model are mostly nonsensical, as the model does not take into account the context of the words. \n",
    "\n",
    "The bigram model generates sentences that are more coherent, but lack variety, since the model only takes into account the previous word.\n",
    "\n",
    "The trigram model generates the most coherent sentences, as it takes into account the context of the previous two words, and is able to generate sentences that are more similar to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b4282-d67c-4543-88cf-1bd0518173c2",
   "metadata": {},
   "source": [
    "## Task3 - Build NB/LR classifiers\n",
    "\n",
    "> 1) Build a Naive Bayes classifier (with Laplace smoothing) and test your model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9cdf1d-8147-4033-bbb0-9147f3647c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 178616\n",
      "Naive Bayes classifier trained.\n",
      "Class log prior shape: (10,)\n",
      "Class word log probability shape: (10, 178616)\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Book       1.00      0.80      0.89        10\n",
      "     Disease       0.71      1.00      0.83        10\n",
      "      Artist       1.00      1.00      1.00        10\n",
      "      Animal       1.00      0.70      0.82        10\n",
      "        Film       0.50      0.90      0.64        10\n",
      "      Writer       0.80      0.80      0.80        10\n",
      "        Food       1.00      0.90      0.95        10\n",
      "  Politician       0.71      1.00      0.83        10\n",
      "       Actor       1.00      0.00      0.00        10\n",
      "    Software       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.87      0.81      0.78       100\n",
      "weighted avg       0.87      0.81      0.78       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load data from a JSON file.\"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Clean and extract texts and labels in the dataset.\"\"\"\n",
    "    texts = [re.sub(r'[^a-zA-Z0-9\\s]', '', record['text']).lower() for record in data]\n",
    "    labels = [record['label'] for record in data]\n",
    "    return texts, labels\n",
    "\n",
    "def create_vocabulary(texts):\n",
    "    \"\"\"Create a set of all unique words in the data.\"\"\"\n",
    "    vocabulary = set()\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            vocabulary.add(word)\n",
    "    return vocabulary\n",
    "\n",
    "def text_to_bow(texts, vocabulary):\n",
    "    \"\"\"Convert a list of texts to a bag-of-words representation.\"\"\"\n",
    "    bow = np.zeros((len(texts), len(vocabulary)))  # row: text, column: word\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    for i, text in enumerate(texts):\n",
    "        for word in text.split():\n",
    "            if word in word_to_index:  # for test set, ignore unknown words\n",
    "                bow[i, word_to_index[word]] += 1\n",
    "    return bow\n",
    "\n",
    "def train_naive_bayes(bow, labels, laplace=1):\n",
    "    \"\"\"Train a Naive Bayes classifier with Laplace smoothing.\"\"\"\n",
    "    n_classes = len(set(labels))  # number of unique classes\n",
    "    n_words = bow.shape[1]  # number of words in the vocabulary\n",
    "    class_word_counts = np.zeros((n_classes, n_words))  # word counts per class\n",
    "    class_counts = np.zeros(n_classes)  # text counts per class\n",
    "    for i, label in enumerate(labels):\n",
    "        index = label_to_index[label]  # convert label to index\n",
    "        class_word_counts[index] += bow[i]\n",
    "        class_counts[index] += 1\n",
    "    class_log_prior = np.log(class_counts) - np.log(np.sum(class_counts))\n",
    "    class_word_log_prob = np.log(class_word_counts + laplace) - np.log(\n",
    "        np.sum(class_word_counts + laplace, axis=1, keepdims=True))  # Laplace smoothing\n",
    "    return class_log_prior, class_word_log_prob\n",
    "\n",
    "def test_naive_bayes(test_bow, class_log_prior, class_word_log_prob):\n",
    "    \"\"\"Predict the class for each example in bow.\"\"\"\n",
    "    log_probs = test_bow @ class_word_log_prob.T + class_log_prior  # matrix, size (n_test, n_classes)\n",
    "    return np.argmax(log_probs, axis=1)  # predicted class for each text, size (n_test,)\n",
    "\n",
    "\n",
    "# load and preprocess the data\n",
    "train_data = load_data('enwiki-train.json')\n",
    "test_data = load_data('enwiki-test.json')\n",
    "train_texts, train_labels = preprocess_data(train_data)\n",
    "test_texts, test_labels = preprocess_data(test_data)\n",
    "unique_labels = list(set(train_labels))  # list of unique labels\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# create vocabulary\n",
    "vocabulary = create_vocabulary(train_texts)\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "# convert texts to bag-of-words representation\n",
    "train_bow = text_to_bow(train_texts, vocabulary)\n",
    "test_bow = text_to_bow(test_texts, vocabulary)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "class_log_prior, class_word_log_prob = train_naive_bayes(\n",
    "    train_bow, np.array(train_labels))\n",
    "\n",
    "print(\"Naive Bayes classifier trained.\")\n",
    "print(\"Class log prior shape:\", class_log_prior.shape)\n",
    "print(\"Class word log probability shape:\", class_word_log_prob.shape)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = test_naive_bayes(test_bow, class_log_prior, class_word_log_prob)\n",
    "\n",
    "# Convert test labels to index\n",
    "test_label_to_index = [label_to_index[lable] for lable in test_labels]\n",
    "label_to_index = {label: i for i, label in enumerate(set(test_labels))}\n",
    "\n",
    "# Evaluate the classifier\n",
    "# accuracy = np.mean(predictions == np.array(test_label_to_index))\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(test_label_to_index, predictions, target_names=label_to_index.keys(), zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca2fab",
   "metadata": {},
   "source": [
    "From the test result above, we can see that the Naive Bayes classifier achieves an accuracy of 0.81 on the test dataset. This means that the classifier correctly predicts the class of 81% of the documents in the test dataset, which is an acceptable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca80a4a-9db5-426f-a45a-b1c2c1e34423",
   "metadata": {},
   "source": [
    "> 2) Build a LR classifier. This question seems to be challenging. We did not directly provide features for samples. But just use your own method to build useful features. You may need to split the training dataset into train and validation so that some involved parameters can be tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd58fd",
   "metadata": {},
   "source": [
    "In the following code, we use TF-IDF as the feature representation for the LR classifier. We use the scikit-learn library to build the it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae83301a-0529-4dc9-899b-504324f9a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01: Validation F1-Score = 0.13\n",
      "C = 0.1: Validation F1-Score = 0.65\n",
      "C = 1: Validation F1-Score = 0.95\n",
      "C = 10: Validation F1-Score = 0.955\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Actor       1.00      1.00      1.00        10\n",
      "      Animal       1.00      0.90      0.95        10\n",
      "      Artist       0.91      1.00      0.95        10\n",
      "        Book       0.77      1.00      0.87        10\n",
      "     Disease       1.00      1.00      1.00        10\n",
      "        Film       1.00      0.90      0.95        10\n",
      "        Food       1.00      1.00      1.00        10\n",
      "  Politician       0.91      1.00      0.95        10\n",
      "    Software       1.00      1.00      1.00        10\n",
      "      Writer       1.00      0.70      0.82        10\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.96      0.95      0.95       100\n",
      "weighted avg       0.96      0.95      0.95       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Load data from JSON files\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load data from a JSON file.\"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "# Split data into features and labels\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Clean and extract texts and labels in the dataset.\"\"\"\n",
    "    texts = [re.sub(r'[^a-zA-Z0-9\\s]', '', record['text']).lower() for record in data]\n",
    "    labels = [record['label'] for record in data]\n",
    "    return texts, labels\n",
    "\n",
    "# Load and prepare training and test data\n",
    "train_data = load_data('enwiki-train.json')\n",
    "test_data = load_data('enwiki-test.json')\n",
    "X_train_full, y_train_full = preprocess_data(train_data)\n",
    "X_test, y_test = preprocess_data(test_data)\n",
    "\n",
    "# Convert labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_full_numeric = label_encoder.fit_transform(y_train_full)\n",
    "y_test_numeric = label_encoder.transform(y_test)\n",
    "\n",
    "# Split the full training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full_numeric, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "C_values = [0.01, 0.1, 1, 10]  # hyperparameter values to test\n",
    "f1_scores = []\n",
    "for C in C_values:\n",
    "    # Train Logistic Regression classifier\n",
    "    lr_model = LogisticRegression(C=C, max_iter=1000, random_state=42)\n",
    "    lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_val_pred = lr_model.predict(X_val_tfidf)\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(y_val, y_val_pred, average='micro')\n",
    "    f1_scores.append(f1)\n",
    "    print(f'C = {C}: Validation F1-Score = {f1}')\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "lr_model = LogisticRegression(C=1, max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test_numeric, y_test_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd079b65",
   "metadata": {},
   "source": [
    "For finding the best regularization parameter, we use grid search with cross-validation. We split the training dataset into training and validation sets, and use the training set to train the LR classifier with different values of the regularization parameter. Notice that a much higher value of C does not give a much better F1-score, so we just stick to the default value of 1.0.\n",
    "\n",
    "The result shows that the LR classifier achieves an accuracy of 0.95 on the test dataset, which is higher than the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8b62-1ef4-4c2f-934d-f78551ad039e",
   "metadata": {},
   "source": [
    "> 3) Report Micro-F1 score and Macro-F1 score for these classifiers on testing dataset explain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500abc8d-0e08-4a84-af6c-9d0e1e0fa035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Micro-F1: 0.8100, Macro-F1: 0.7769\n",
      "Logistic Regression - Micro-F1: 0.9500, Macro-F1: 0.9493\n"
     ]
    }
   ],
   "source": [
    "# Your code goes to here\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "micro_f1_nb = f1_score(test_label_to_index, predictions, average='micro')\n",
    "macro_f1_nb = f1_score(test_label_to_index, predictions, average='macro')\n",
    "\n",
    "micro_f1_lr = f1_score(y_test_numeric, y_test_pred, average='micro')\n",
    "macro_f1_lr = f1_score(y_test_numeric, y_test_pred, average='macro')\n",
    "\n",
    "print(\n",
    "    f\"Naive Bayes - Micro-F1: {micro_f1_nb:.4f}, Macro-F1: {macro_f1_nb:.4f}\")\n",
    "\n",
    "print(\n",
    "    f\"Logistic Regression - Micro-F1: {micro_f1_lr:.4f}, Macro-F1: {macro_f1_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47f7ca",
   "metadata": {},
   "source": [
    "From the results above, LR classifier outperforms the NB classifier in terms of both Micro-F1 and Macro-F1 scores. A higher Micro-F1 score suggests that the model performs well on the majority class but not necessarily on minority classes. A higher Macro-F1 score indicates more balanced performance across all classes but can be lowered by poor performance on any single class.\n",
    "\n",
    "An explanation is that LR is better suited for this specific dataset as a linear model, while NB works well with small datasets and achieves good results even with the presence of irrelevant features. LR's performance also might benefit from the TF-IDF representation, as it can leverage the importance of features in distinguishing between classes, which is reflected in the F1 scores. Thus, LR becomes as a more powerful model than NB, and it can capture more complex relationships between features and labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
