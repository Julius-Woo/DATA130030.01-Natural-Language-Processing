{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a067b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Before your go ----\n",
    "# 1. Rename Assignment-03-###.ipynb where ### is your student ID.\n",
    "# 2. The deadline of Assignment-03 is 23:59pm, 06-05-2024\n",
    "\n",
    "\n",
    "# --- Explore HMM POS Taggers using Brown corpus ---\n",
    "# In this assignment, you will explore three taggers for a Brown corpus.\n",
    "# import your packages here\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist, LidstoneProbDist\n",
    "from nltk.tag import HiddenMarkovModelTagger\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb55db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 Tags: ['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "# Task 1 --- Load and explore your data ---\n",
    "# 1). load train/test samples from Brown corpus files, brown-train.txt, brown-test.txt.\n",
    "# 2). load all 12 tags from brown-tag.txt and print it out\n",
    "# 3). counting how many sentences and words in both train and test datasets.\n",
    "# 4). for each tag, counting how many words in train and test. e.g, tag1: [count_tr, count_te]\n",
    "\n",
    "# Your code\n",
    "train_file = 'brown-train.txt'\n",
    "test_file = 'brown-test.txt'\n",
    "tag_file = 'brown-tag.txt'\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read().strip().split('\\n\\n')  # read as a string and split by double newline\n",
    "    sentences = []\n",
    "    for sentence in data:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        id = lines[0]  # first line is the sentence id\n",
    "        word_tag_pairs = lines[1:]  # the rest of the lines are word/tag pairs\n",
    "        sentences.append([tuple(word_tag.split('\\t'))\n",
    "                         for word_tag in word_tag_pairs])\n",
    "    return sentences\n",
    "\n",
    "# 1) load data\n",
    "train_sentences = load_data(train_file)\n",
    "test_sentences = load_data(test_file)\n",
    "\n",
    "# 2) load tags\n",
    "with open(tag_file, 'r') as file:\n",
    "    tags = file.read().strip().split()\n",
    "\n",
    "print(\"12 Tags:\", tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bddfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train dataset: 45800\n",
      "Number of words in train dataset (punctuation included): 928327\n",
      "Number of unique words in train dataset (punctuation included): 50490\n",
      "Number of sentences in test dataset: 11540\n",
      "Number of words in test dataset (punctuation included): 232865\n",
      "Number of unique words in test dataset (punctuation included): 25353\n"
     ]
    }
   ],
   "source": [
    "# 3) counting sentences and words\n",
    "num_train_sentences = len(train_sentences)\n",
    "num_test_sentences = len(test_sentences)\n",
    "\n",
    "num_train_words = sum(len(sentence) for sentence in train_sentences)\n",
    "num_test_words = sum(len(sentence) for sentence in test_sentences)\n",
    "unique_train_words = set(word for sentence in train_sentences for word, _ in sentence)\n",
    "unique_test_words = set(word for sentence in test_sentences for word, _ in sentence)\n",
    "num_unique_train_words = len(unique_train_words)\n",
    "num_unique_test_words = len(unique_test_words)\n",
    "\n",
    "print(f\"Number of sentences in train dataset: {num_train_sentences}\")\n",
    "print(f\"Number of words in train dataset (punctuation included): {num_train_words}\")\n",
    "print(f\"Number of unique words in train dataset (punctuation included): {num_unique_train_words}\")\n",
    "print(f\"Number of sentences in test dataset: {num_test_sentences}\")\n",
    "print(f\"Number of words in test dataset (punctuation included): {num_test_words}\")\n",
    "print(f\"Number of unique words in test dataset (punctuation included): {num_unique_test_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f34509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag counts [train, test]:\n",
      ".: [117723, 29842]\n",
      "ADJ: [66985, 16736]\n",
      "ADP: [115752, 29014]\n",
      "ADV: [44765, 11474]\n",
      "CONJ: [30455, 7696]\n",
      "DET: [109418, 27601]\n",
      "NOUN: [220451, 55107]\n",
      "NUM: [11921, 2953]\n",
      "PRON: [39657, 9677]\n",
      "PRT: [23889, 5940]\n",
      "VERB: [146199, 36551]\n",
      "X: [1112, 274]\n"
     ]
    }
   ],
   "source": [
    "# 4) count words for each tag\n",
    "def count_tags(sentences):\n",
    "    tags = []\n",
    "    for sentence in sentences:\n",
    "        for pair in sentence:\n",
    "            tags.append(pair[1])\n",
    "    return FreqDist(tags)\n",
    "\n",
    "\n",
    "train_tag_freq = count_tags(train_sentences)\n",
    "test_tag_freq = count_tags(test_sentences)\n",
    "\n",
    "tag_word_count = {tag: [train_tag_freq[tag],\n",
    "                        test_tag_freq[tag]] for tag in tags}\n",
    "\n",
    "print(\"Tag counts [train, test]:\")\n",
    "for tag, counts in tag_word_count.items():\n",
    "    print(f\"{tag}: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d633df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 --- Method 1: Build a baseline method, namely, the most frequent tagger ---\n",
    "#     If you can recall, we introduced a strong baseline method (See Dan's book in \n",
    "# https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf Page 164.),\n",
    "#     where we label each word by using the most frequent-used tag associated with it.\n",
    "# 1). find the most frequent class label for each word in the training data.\n",
    "#     For example, {tr_word_1:tag_1,tr_word_2:tag_2,...}\n",
    "# 2). use your built method to predict tags for both train and test datasets.\n",
    "#     You should print out two values: the accuracies of train and test samples.\n",
    "#     You would expect that the accuracy on train will be > 9.0 (but never = 1.0) and higher than on test.\n",
    "\n",
    "# Notice: since there are unkown words in test samples. \n",
    "#  Following ways could handle this (choose one or create your own): \n",
    "#  1). mark all words that appear only once in the data with a \"UNK-x\" tag\n",
    "#  2). tag every out-of-vocabulary word with the majority tag among all training samples.\n",
    "#  3). find more methods in https://github.com/Adamouization/POS-Tagging-and-Unknown-Words\n",
    "\n",
    "# Your code\n",
    "# 1) find the most frequent class label for each word\n",
    "def find_mostfreq_tag_word(dataset):\n",
    "    word_tag_freq = defaultdict(Counter)\n",
    "    for sentence in dataset:\n",
    "        for word, tag in sentence:\n",
    "            word_tag_freq[word][tag] += 1\n",
    "    most_freq_tag = {word: tag_freq.most_common(1)[0][0]\n",
    "                     for word, tag_freq in word_tag_freq.items()}\n",
    "    return most_freq_tag\n",
    "\n",
    "\n",
    "# 2) predict tags\n",
    "def predict_tags(sentences, most_freq_tag, default_tag='NOUN'):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sentence in sentences:\n",
    "        for word, true_tag in sentence:\n",
    "            pred_tag = most_freq_tag.get(word, default_tag)\n",
    "            if pred_tag == true_tag:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c66dca",
   "metadata": {},
   "source": [
    "Handling UNK method 1: replace with \"UNK\" token\n",
    "\n",
    "Part of the UNK handling functions below are taken from https://github.com/Adamouization/POS-Tagging-and-Unknown-Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d1b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle UNKs: mark all words that appear only once in the data with a \"UNK-x\" tag\n",
    "def extract_words(dataset):\n",
    "    # dataset: list of sentences, where each sentence is a list of (word, tag) tuples\n",
    "    return [word for sentence in dataset for word, _ in sentence]\n",
    "\n",
    "\n",
    "def get_hapax_words(words, threshold=1):\n",
    "    # Return words that appear only once in the data\n",
    "    words_freq = FreqDist(words)\n",
    "    return [word for word, freq in words_freq.items() if freq <= threshold]\n",
    "\n",
    "\n",
    "# def basic_UNK_rules(word):\n",
    "#     if word.endswith('ing'):\n",
    "#         return \"UNK-ing\"\n",
    "#     elif word.istitle():\n",
    "#         return \"UNK-capitalised\"\n",
    "#     return \"UNK\"\n",
    "\n",
    "def extra_UNK_rules(word):\n",
    "    if word.startswith('$'):\n",
    "        return \"UNK-currency\"\n",
    "    elif word.isdigit():\n",
    "        return \"UNK-number\"\n",
    "    elif re.compile(r'\\d+(?:[,.]\\d*)?').match(word):\n",
    "        return \"UNK-decimal-number\"\n",
    "    elif word.istitle():\n",
    "        if word.endswith('ing'):\n",
    "            return \"Unk-ing\"\n",
    "        elif word.endswith('ed'):\n",
    "            return \"Unk-ed\"\n",
    "        elif word.endswith(\"'s\"):\n",
    "            return \"Unk-apostrophe-s\"\n",
    "        elif '-' in word:\n",
    "            return \"Unk-hyphen\"\n",
    "    elif not word.istitle():\n",
    "        if word.endswith('ing'):\n",
    "            return \"unk-ing\"\n",
    "        elif word.endswith('ed'):\n",
    "            return \"unk-ed\"\n",
    "        elif word.endswith(\"'s\"):\n",
    "            return \"unk-apostrophe-s\"\n",
    "        elif '-' in word:\n",
    "            return \"unk-hyphen\"\n",
    "    return \"UNK\"\n",
    "\n",
    "\n",
    "def handle_UNK_train(dataset, hapax_words):\n",
    "    # Replace hapax words with appropriate \"UNK-x\" strings\n",
    "    for sentence in tqdm(dataset, desc=\"Replacing train words\", total=len(dataset), leave=True):\n",
    "        for i, (word, tag) in enumerate(sentence):\n",
    "            if word in hapax_words:\n",
    "                sentence[i] = (extra_UNK_rules(word), tag)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def handle_UNK_test(dataset, unique_train_words):\n",
    "    # Replace test words not in the training set\n",
    "    for sentence in tqdm(dataset, desc=\"Replacing test words\", total=len(dataset), leave=True):\n",
    "        for i, (word, tag) in enumerate(sentence):\n",
    "            if word not in unique_train_words:\n",
    "                sentence[i] = (extra_UNK_rules(word), tag)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50b8d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing test words: 100%|██████████| 11540/11540 [00:00<00:00, 256419.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish handling UNK words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_hapax_words = get_hapax_words(extract_words(train_sentences))\n",
    "train_sentences_UNK = handle_UNK_train(train_sentences, train_hapax_words)\n",
    "\n",
    "# update unique words (remove hapax words)\n",
    "unique_train_words_hapax_excluded = unique_test_words - set(train_hapax_words)\n",
    "test_sentences_UNK = handle_UNK_test(\n",
    "    test_sentences, unique_train_words_hapax_excluded)\n",
    "print(\"Finish handling UNK words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53718b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('train_sentences_UNK.pkl', 'wb') as file:\n",
    "    pickle.dump(train_sentences_UNK, file)\n",
    "\n",
    "with open('test_sentences_UNK.pkl', 'wb') as file:\n",
    "    pickle.dump(test_sentences_UNK, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31692ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "with open('train_sentences_UNK.pkl', 'rb') as file:\n",
    "    train_sentences_UNK = pickle.load(file)\n",
    "\n",
    "with open('test_sentences_UNK.pkl', 'rb') as file:\n",
    "    test_sentences_UNK = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d2d3d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline method with UNK replacement handling:\n",
      "Train accuracy: 0.9495\n",
      "Test accuracy: 0.9437\n"
     ]
    }
   ],
   "source": [
    "# Perform baseline method with UNK replacement handling\n",
    "most_freq_tag_UNK = find_mostfreq_tag_word(train_sentences_UNK)\n",
    "\n",
    "baselineUNK_train_acc = predict_tags(train_sentences_UNK, most_freq_tag_UNK)\n",
    "baselineUNK_test_acc = predict_tags(test_sentences_UNK, most_freq_tag_UNK)\n",
    "\n",
    "print(f\"Baseline method with UNK replacement handling:\")\n",
    "print(f\"Train accuracy: {baselineUNK_train_acc:.4f}\")\n",
    "print(f\"Test accuracy: {baselineUNK_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2897a88",
   "metadata": {},
   "source": [
    "Handling UNK method 2: tagging with the most frequent tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f502e427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline method with UNK majority tag handling:\n",
      "Train Accuracy: 0.9572\n",
      "Test Accuracy: 0.9452\n"
     ]
    }
   ],
   "source": [
    "# handle UNKs: tag every out-of-vocabulary word with the majority tag among all training sample\n",
    "default_tag = train_tag_freq.most_common(1)[0][0]\n",
    "most_freq_tag = find_mostfreq_tag_word(train_sentences)\n",
    "\n",
    "baselineMaj_train_acc = predict_tags(train_sentences, most_freq_tag, default_tag)\n",
    "baselineMaj_test_acc = predict_tags(test_sentences, most_freq_tag, default_tag)\n",
    "\n",
    "print(f\"Baseline method with majority tag handling:\")\n",
    "print(f\"Train Accuracy: {baselineMaj_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {baselineMaj_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e38802c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\nlp-24\\lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "d:\\anaconda3\\envs\\nlp-24\\lib\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy over 928327 tokens: 96.85\n",
      "accuracy over 232865 tokens: 96.09\n",
      "Baseline Train Accuracy: 0.9495\n",
      "Baseline Test Accuracy: 0.9437\n"
     ]
    }
   ],
   "source": [
    "# Task 3 --- Method 2: Build an HMM tagger ---\n",
    "# 1) You should use nltk.tag.HiddenMarkovModelTagger to build an HMM tagger.\n",
    "#    It has parameters: symbols, states, transitions, outputs, priors, transform (ignore it).\n",
    "#    Specify these parameters properly. For example, you can use MLE to estimate transitions, outputs and priors.\n",
    "#    That is, MLE to estimate matrix A (transition matrix), and matrix B (output probabilites) (See. Page 8.4.3)\n",
    "# 2) After build your model, report both the accuracy of HMM tagger for train samples and test samples.\n",
    "# \n",
    "# 3) Compared with your baseline method, discuss that why your HMM tagger is better/worse than baseline method.\n",
    "\n",
    "# Notice: You may also need to handle unknown words just like Task 2.\n",
    "\n",
    "# Your code\n",
    "# 1) build HMM tagger\n",
    "# Calculate transition probabilities (A), output probabilities (B), and priors\n",
    "def get_HMM_params(dataset):\n",
    "    symbols = set(word for sentence in train_sentences for word, _ in sentence)\n",
    "    states = set(tag for sentence in train_sentences for _, tag in sentence)\n",
    "    transitions = []\n",
    "    outputs = []\n",
    "    priors = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        previous_tag = None\n",
    "        for word, tag in sentence:\n",
    "            outputs.append((tag, word))\n",
    "            if previous_tag is None:\n",
    "                priors.append(tag)\n",
    "            else:\n",
    "                transitions.append((previous_tag, tag))\n",
    "            previous_tag = tag\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    A = ConditionalProbDist(ConditionalFreqDist(transitions), MLEProbDist)\n",
    "    B = ConditionalProbDist(ConditionalFreqDist(outputs), MLEProbDist)\n",
    "    priors = MLEProbDist(FreqDist(priors))\n",
    "\n",
    "    return symbols, states, A, B, priors\n",
    "\n",
    "\n",
    "symbols, states, A, B, priors = get_HMM_params(train_sentences_UNK)\n",
    "hmm_tagger = HiddenMarkovModelTagger(\n",
    "    symbols=symbols, states=states, transitions=A, outputs=B, priors=priors)\n",
    "\n",
    "# 2) evaluate HMM tagger\n",
    "hmm_tagger.test(train_sentences_UNK)\n",
    "hmm_tagger.test(test_sentences_UNK)\n",
    "\n",
    "# 3) compare with baseline\n",
    "print(f\"Baseline Train Accuracy: {baselineUNK_train_acc:.4f}\")\n",
    "print(f\"Baseline Test Accuracy: {baselineUNK_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777e01c",
   "metadata": {},
   "source": [
    "We observe that there is an issue of overflow. Now we try to perform smoothing to avoid the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631a279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy over 928327 tokens: 95.81\n",
      "accuracy over 232865 tokens: 95.13\n"
     ]
    }
   ],
   "source": [
    "def get_HMM_params_smooth(dataset, gamma=0.1):\n",
    "    symbols = set(word for sentence in train_sentences for word, _ in sentence)\n",
    "    states = set(tag for sentence in train_sentences for _, tag in sentence)\n",
    "    transitions = []\n",
    "    outputs = []\n",
    "    priors = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        previous_tag = None\n",
    "        for word, tag in sentence:\n",
    "            outputs.append((tag, word))\n",
    "            if previous_tag is None:\n",
    "                priors.append(tag)\n",
    "            else:\n",
    "                transitions.append((previous_tag, tag))\n",
    "            previous_tag = tag\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    A = ConditionalProbDist(ConditionalFreqDist(transitions), lambda fd: LidstoneProbDist(fd, gamma))\n",
    "    B = ConditionalProbDist(ConditionalFreqDist(outputs), lambda fd: LidstoneProbDist(fd, gamma))\n",
    "    priors = LidstoneProbDist(FreqDist(priors), gamma)\n",
    "\n",
    "    return symbols, states, A, B, priors\n",
    "\n",
    "\n",
    "symbols, states, A, B, priors = get_HMM_params_smooth(train_sentences_UNK)\n",
    "hmm_tagger = HiddenMarkovModelTagger(symbols=symbols, states=states, transitions=A, outputs=B, priors=priors)\n",
    "\n",
    "# 2) evaluate HMM tagger\n",
    "hmm_tagger.test(train_sentences_UNK)\n",
    "hmm_tagger.test(test_sentences_UNK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f1067",
   "metadata": {},
   "source": [
    "Although Lidstone smoothing lowers the accuracy, it is still better than the baseline method. \n",
    "\n",
    "The HMM tagger performs better than the baseline method because it considers the context in which words appear. By using transition probabilities to model the likelihood of a tag given the previous tag, the HMM tagger captures the syntactic structure of the language. This allows it to disambiguate words based on their context, improving accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeca6c5",
   "metadata": {},
   "source": [
    "For Task 4 below, some codes are from https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58ab053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 --- Method 3: Fine-tuning on BERT-base model for POS-tagging ---\n",
    "# \n",
    "# 1) You may download a BERT model (say, you choose BERT-base cased) \n",
    "#    and use tools in https://github.com/huggingface/transformers\n",
    "# \n",
    "# 2) After build your model, report both the accuracy of BERT tagger for train samples and test samples.\n",
    "# \n",
    "# 3) Compared with Method 1,2, discuss that why your BERT tagger is better/worse than these two.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tag2id = {tag: i for i, tag in enumerate(tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "\n",
    "# convert data into datasets.Dataset format\n",
    "def convert_dataset(sentences):\n",
    "    words = [[word for word, tag in sentence] for sentence in sentences]\n",
    "    tags = [[tag2id[tag] for word, tag in sentence] for sentence in sentences]\n",
    "    return {'words': words, 'tags': tags}\n",
    "\n",
    "\n",
    "train_data = convert_dataset(train_sentences)\n",
    "test_data = convert_dataset(test_sentences)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d7c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "# BERT uses subword tokenization, so we need to tokenize the words and align the labels\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a74982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize lots of texts at the same time\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"words\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87d82e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Flatten the arrays to match the format required by classification_report\n",
    "    true_labels = np.concatenate(labels, axis=0)\n",
    "    true_preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    mask = true_labels != -100\n",
    "    true_labels = true_labels[mask]\n",
    "    true_preds = true_preds[mask]\n",
    "\n",
    "    report_dict = classification_report(true_labels, true_preds, target_names=tags, output_dict=True)\n",
    "    report_str = classification_report(true_labels, true_preds, target_names=tags)\n",
    "\n",
    "    print(report_str) \n",
    "    # with open(\"classification_report.txt\", \"w\") as f:\n",
    "    #     f.write(report_str)\n",
    "    accuracy = report_dict[\"accuracy\"]\n",
    "    macro_f1 = report_dict[\"macro avg\"][\"f1-score\"]\n",
    "    weighted_f1 = report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b602bfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd043b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5725' max='5725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5725/5725 06:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5725, training_loss=0.07529560772092062, metrics={'train_runtime': 371.0463, 'train_samples_per_second': 123.435, 'train_steps_per_second': 15.429, 'total_flos': 1180980295456896.0, 'train_loss': 0.07529560772092062, 'epoch': 1.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir='results',\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=500\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b248a174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       1.00      1.00      1.00    134478\n",
      "         ADJ       0.98      0.98      0.98     87581\n",
      "         ADP       0.99      0.99      0.99    115959\n",
      "         ADV       0.98      0.98      0.98     49180\n",
      "        CONJ       1.00      1.00      1.00     30495\n",
      "         DET       1.00      1.00      1.00    109463\n",
      "        NOUN       0.99      0.99      0.99    296657\n",
      "         NUM       0.99      0.99      0.99     14695\n",
      "        PRON       1.00      0.99      0.99     39762\n",
      "         PRT       0.98      0.99      0.99     28383\n",
      "        VERB       0.99      1.00      1.00    165283\n",
      "           X       0.92      0.78      0.84      2400\n",
      "\n",
      "    accuracy                           0.99   1074336\n",
      "   macro avg       0.98      0.97      0.98   1074336\n",
      "weighted avg       0.99      0.99      0.99   1074336\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       1.00      1.00      1.00     34144\n",
      "         ADJ       0.96      0.97      0.97     22154\n",
      "         ADP       0.99      0.99      0.99     29075\n",
      "         ADV       0.97      0.97      0.97     12774\n",
      "        CONJ       1.00      1.00      1.00      7702\n",
      "         DET       1.00      1.00      1.00     27614\n",
      "        NOUN       0.99      0.99      0.99     74019\n",
      "         NUM       0.99      0.99      0.99      3694\n",
      "        PRON       0.99      0.99      0.99      9702\n",
      "         PRT       0.98      0.97      0.98      7092\n",
      "        VERB       0.99      0.99      0.99     41313\n",
      "           X       0.87      0.75      0.80       616\n",
      "\n",
      "    accuracy                           0.99    269899\n",
      "   macro avg       0.98      0.97      0.97    269899\n",
      "weighted avg       0.99      0.99      0.99    269899\n",
      "\n",
      "Training results of BERT:\n",
      " {'eval_loss': 0.024829400703310966, 'eval_accuracy': 0.992826266642838, 'eval_macro_f1': 0.979144713498989, 'eval_weighted_f1': 0.9928016664537355, 'eval_runtime': 72.3, 'eval_samples_per_second': 633.472, 'eval_steps_per_second': 79.184, 'epoch': 1.0}\n",
      "Test results of BERT:\n",
      " {'eval_loss': 0.03890230506658554, 'eval_accuracy': 0.9885920288700588, 'eval_macro_f1': 0.9718149544361493, 'eval_weighted_f1': 0.9885650405261078, 'eval_runtime': 17.0555, 'eval_samples_per_second': 676.613, 'eval_steps_per_second': 84.606, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "train_results_BERT = trainer.evaluate(eval_dataset=tokenized_train_dataset)\n",
    "test_results_BERT = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "print(\"Training results of BERT:\\n\", train_results_BERT)\n",
    "print(\"Test results of BERT:\\n\", test_results_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef53280",
   "metadata": {},
   "source": [
    "We can summarize the result of every method into a table:\n",
    "\n",
    "| Tagger | Accuracy on trainset | Accuracy on testset |\n",
    "| --- | --- | --- |\n",
    "| Baseline+MostFreq | 95.72 | 94.52 |\n",
    "| Baseline+UNK | 94.95 | 94.37 |\n",
    "| HMM+UNK+MLE | 96.85 | 96.09 |\n",
    "| HMM+UNK+MLE+Lidstone | 95.81 | 95.13 |\n",
    "| **BERT** (1 epoch) | **99.28** | **98.86** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10b434",
   "metadata": {},
   "source": [
    "Our fine-tuned BERT outperforms baseline and HMM in POS tagging, probably due to its advanced architecture and capabilities:\n",
    "\n",
    "1. BERT captures the context of each word within a sentence, allowing it to understand word meanings based on surrounding words. This is crucial for POS tagging as the meaning of a word can change depending on its context.\n",
    "\n",
    "2. BERT leverages deep neural networks with multiple layers, enabling it to learn complex patterns and representations in language data, leading to higher accuracy.\n",
    "\n",
    "3. BERT is pre-trained on a large corpus of text using masked language modeling and next sentence prediction, which helps it to grasp general language structures and nuances effectively.\n",
    "\n",
    "4. The self-attention mechanism in BERT's transformer architecture allows it to weigh the importance of different words in a sentence, which can enhance its ability to make accurate predictions.\n",
    "\n",
    "In contrast, baseline methods like most frequent tagging do not consider context, and HMMs, while considering context, are limited by their simpler statistical models and inability to capture long-range dependencies effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
